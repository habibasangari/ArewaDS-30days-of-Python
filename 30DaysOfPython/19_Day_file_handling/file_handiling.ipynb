{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lines_and_words(filename):\n",
    "    line_count = 0\n",
    "    word_count = 0\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                line_count += 1\n",
    "                words = line.split()\n",
    "                word_count += len(words)\n",
    "\n",
    "    return line_count, word_count\n",
    "\n",
    "filename = './data/obama_speech.txt'\n",
    "\n",
    "lines, words = count_lines_and_words(filename)\n",
    "\n",
    "print(\"Number of lines:\", lines)\n",
    "print(\"Number of words:\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lines_and_words(filename):\n",
    "    line_count = 0\n",
    "    word_count = 0\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                line_count += 1\n",
    "                words = line.split()\n",
    "                word_count += len(words)\n",
    "\n",
    "    return line_count, word_count\n",
    "\n",
    "filename = './data/michelle_obama_speech.txt'\n",
    "\n",
    "lines, words = count_lines_and_words(filename)\n",
    "\n",
    "print(\"Number of lines:\", lines)\n",
    "print(\"Number of words:\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lines_and_words(filename):\n",
    "    line_count = 0\n",
    "    word_count = 0\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                line_count += 1\n",
    "                words = line.split()\n",
    "                word_count += len(words)\n",
    "\n",
    "    return line_count, word_count\n",
    "\n",
    "filename = './data/donald_speech.txt'\n",
    "\n",
    "lines, words = count_lines_and_words(filename)\n",
    "\n",
    "print(\"Number of lines:\", lines)\n",
    "print(\"Number of words:\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lines_and_words(filename):\n",
    "    line_count = 0\n",
    "    word_count = 0\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                line_count += 1\n",
    "                words = line.split()\n",
    "                word_count += len(words)\n",
    "\n",
    "    return line_count, word_count\n",
    "\n",
    "filename = './data/melina_trump_speech.txt'\n",
    "\n",
    "lines, words = count_lines_and_words(filename)\n",
    "\n",
    "print(\"Number of lines:\", lines)\n",
    "print(\"Number of words:\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def find_most_spoken_languages():\n",
    "    with open('./data/countries_data.json', 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    language_count = {}\n",
    "\n",
    "    for country in data:\n",
    "        languages = country['languages']\n",
    "\n",
    "        for language in languages:\n",
    "            language_count[language] = language_count.get(language, 0) + 1\n",
    "\n",
    "    sorted_languages = sorted(language_count.items(), key=lambda x: x[1], reverse=True)\n",
    "    most_spoken_languages = [language for language, count in sorted_languages[:10]]\n",
    "\n",
    "    return most_spoken_languages\n",
    "\n",
    "most_spoken = find_most_spoken_languages()\n",
    "\n",
    "print(\"Ten most spoken languages:\")\n",
    "for language in most_spoken:\n",
    "    print(language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Read the countries_data.json data file in data directory, create a function that creates a list of the ten \n",
    "# most populated countries\n",
    "import json\n",
    "\n",
    "def find_most_populated_countries():\n",
    "    with open('./data/countries_data.json', 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    sorted_countries = sorted(data, key=lambda x: x['population'], reverse=True)\n",
    "    most_populated_countries = [country['name'] for country in sorted_countries[:10]]\n",
    "\n",
    "    return most_populated_countries\n",
    "\n",
    "most_populated = find_most_populated_countries()\n",
    "\n",
    "print(\"Ten most populated countries:\")\n",
    "for country in most_populated:\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_incoming_email_addresses(filename):\n",
    "    email_addresses = set()\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.lower().startswith(\"from:\"):\n",
    "                match = re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b', line)\n",
    "                if match:\n",
    "                    email_addresses.add(match.group())\n",
    "\n",
    "    return email_addresses\n",
    "\n",
    "def print_email_addresses(addresses):\n",
    "    print(\"Incoming email addresses:\")\n",
    "    for address in addresses:\n",
    "        print(address)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filename = './email_exchange_big.txt'\n",
    "    addresses = extract_incoming_email_addresses(filename)\n",
    "    print_email_addresses(addresses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def find_most_common_words(data, num_words):\n",
    "    # Load the list of common English words\n",
    "    with open('./common_words_frequency.txt', 'r') as file:\n",
    "        common_words = {word.strip().lower() for word in file}\n",
    "\n",
    "    # Initialize a counter to keep track of word frequencies\n",
    "    word_counter = Counter()\n",
    "\n",
    "    # Process the data based on its type: string or file\n",
    "    if isinstance(data, str):\n",
    "        words = re.findall(r'\\b\\w+\\b', data.lower())\n",
    "        word_counter.update(words)\n",
    "    else:\n",
    "        for line in data:\n",
    "            words = re.findall(r'\\b\\w+\\b', line.lower())\n",
    "            word_counter.update(words)\n",
    "\n",
    "    # Filter out non-common words and sort by frequency in descending order\n",
    "    common_words_freq = [(word, freq) for word, freq in word_counter.items() if word in common_words]\n",
    "    common_words_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the specified number of most common words\n",
    "    return common_words_freq[:num_words]\n",
    "\n",
    "# Example usage with a string\n",
    "text = \"This is the source of our confidence - the knowledge that God calls on us to shape an uncertain destiny.\"\n",
    "most_common = find_most_common_words(text, 5)\n",
    "print(\"Most common words in the text:\")\n",
    "for word, freq in most_common:\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n",
    "# Example usage with a file\n",
    "filename = './sample_text.txt'\n",
    "with open(filename, 'r') as file:\n",
    "    most_common = find_most_common_words(file, 10)\n",
    "    print(\"\\nMost common words in the file:\")\n",
    "    for word, freq in most_common:\n",
    "        print(f\"{word}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def find_most_frequent_words(data, num_words):\n",
    "    words = re.findall(r'\\b\\w+\\b', data.lower())\n",
    "    word_counter = Counter(words)\n",
    "    most_frequent_words = word_counter.most_common(num_words)\n",
    "    return most_frequent_words\n",
    "\n",
    "# Read Obama's speech from the file\n",
    "filename = './data/obama_speech.txt'\n",
    "with open(filename, 'r', encoding='utf-8') as file:\n",
    "    speech_text = file.read()\n",
    "\n",
    "# Find the ten most frequent words in Obama's speech\n",
    "most_frequent_words = find_most_frequent_words(speech_text, 10)\n",
    "\n",
    "# Print the ten most frequent words\n",
    "print(\"The ten most frequent words in Obama's speech:\")\n",
    "for word, frequency in most_frequent_words:\n",
    "    print(f\"{word}: {frequency}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def find_most_frequent_words(text, top_n=10):\n",
    "    # Preprocess the text by removing non-alphanumeric characters and converting to lowercase\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "    # Use Counter to count the occurrences of each word\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # Get the most common words\n",
    "    most_common_words = word_counts.most_common(top_n)\n",
    "\n",
    "    return most_common_words\n",
    "\n",
    "# Your Michelle Obama speech text\n",
    "speech_text = \"\"\"\n",
    "   ... (the speech text) ...\n",
    "\"\"\"\n",
    "\n",
    "# Find the ten most frequent words in the speech\n",
    "most_frequent_words = find_most_frequent_words(speech_text, top_n=10)\n",
    "\n",
    "# Print the result\n",
    "for word, count in most_frequent_words:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "c) The ten most frequent words used in [Trump's speech]\n",
    "(https://github.com/Asabeneh/30-Days-Of-Python/blob/master/data/donald_speech.txt)\n",
    "\"\"\"\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def find_most_common_words(data, num_words):\n",
    "    words = re.findall(r'\\b\\w+\\b', data.lower())\n",
    "    word_counter = Counter(words)\n",
    "    common_words = [(word, freq) for word, freq in word_counter.items()]\n",
    "    common_words.sort(key=lambda x: x[1], reverse=True)\n",
    "    return common_words[:num_words]\n",
    "\n",
    "# Read Obama's speech from the file\n",
    "filename = './data/donald_speech.txt'\n",
    "with open(filename, 'r') as file:\n",
    "    speech_text = file.read()\n",
    "\n",
    "# Find the ten most frequent words in Obama's speech\n",
    "most_frequent_words = find_most_common_words(speech_text, 10)\n",
    "\n",
    "# Print the ten most frequent words\n",
    "for word, frequency in most_frequent_words:\n",
    "    print(word, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "c) The ten most frequent words used in [Melina's speech]\n",
    "(https://github.com/Asabeneh/30-Days-Of-Python/blob/master/data/Melina_donald_speech.txt)\n",
    "\"\"\"\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def find_most_common_words(data, num_words):\n",
    "    words = re.findall(r'\\b\\w+\\b', data.lower())\n",
    "    word_counter = Counter(words)\n",
    "    common_words = [(word, freq) for word, freq in word_counter.items()]\n",
    "    common_words.sort(key=lambda x: x[1], reverse=True)\n",
    "    return common_words[:num_words]\n",
    "\n",
    "# Read Obama's speech from the file\n",
    "filename = './data/donald_speech.txt'\n",
    "with open(filename, 'r') as file:\n",
    "    speech_text = file.read()\n",
    "\n",
    "# Find the ten most frequent words in Obama's speech\n",
    "most_frequent_words = find_most_common_words(speech_text, 10)\n",
    "\n",
    "# Print the ten most frequent words\n",
    "for word, frequency in most_frequent_words:\n",
    "    print(word, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "7. Write a python application that checks similarity between two texts. It takes a file or a string as a parameter and \n",
    "it will evaluate the similarity of the two texts. For instance check the similarity between the transcripts \n",
    "of [Michelle's](https://github.com/Asabeneh/30-Days-Of-Python/blob/master/data/michelle_obama_speech.txt) and \n",
    "[Melina's](https://github.com/Asabeneh/30-Days-Of-Python/blob/master/data/melina_trump_speech.txt) speech. \n",
    "You may need a couple of functions, function to clean the text(clean_text), function to remove support \n",
    "words(remove_support_words) and finally to check the similarity(check_text_similarity). List of [stop words]\n",
    "(https://github.com/Asabeneh/30-Days-Of-Python/blob/master/data/stop_words.py) are in the data directory\n",
    "\n",
    "\"\"\"\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Remove whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word.casefold() not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def check_text_similarity(text1, text2):\n",
    "    # Clean and preprocess the texts\n",
    "    clean_text1 = clean_text(text1)\n",
    "    clean_text2 = clean_text(text2)\n",
    "\n",
    "    # Remove stop words\n",
    "    clean_text1 = remove_stop_words(clean_text1)\n",
    "    clean_text2 = remove_stop_words(clean_text2)\n",
    "\n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the vectorizer on the texts\n",
    "    tfidf_matrix = vectorizer.fit_transform([clean_text1, clean_text2])\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "    return similarity_score\n",
    "\n",
    "# Read the speech texts from files\n",
    "michelle_speech_path = \"./data/michelle_obama_speech.txt\"\n",
    "melania_speech_path = \"./data/melania_trump_speech.txt\"\n",
    "\n",
    "with open('./data/michelle_obama_speech.txt', \"r\", encoding=\"utf-8\") as file:\n",
    "    michelle_speech = file.read()\n",
    "\n",
    "with open('./data/melina_trump_speech.txt', \"r\", encoding=\"utf-8\") as file:\n",
    "    melania_speech = file.read()\n",
    "\n",
    "# Calculate the similarity score\n",
    "similarity_score = check_text_similarity(michelle_speech, melania_speech)\n",
    "print(f\"The similarity score between the two speeches is: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Find the 10 most repeated words in the romeo_and_juliet.txt\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Read the content of the file\n",
    "with open('./data/romeo_and_juliet.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Remove punctuation and convert to lowercase\n",
    "content = re.sub(r'[^\\w\\s]', '', content.lower())\n",
    "\n",
    "# Split the content into words\n",
    "words = content.split()\n",
    "\n",
    "# Count the occurrences of each word\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Get the 10 most common words and their frequencies\n",
    "top_10_words = word_counts.most_common(10)\n",
    "\n",
    "# Print the results\n",
    "print(\"The 10 most repeated words in 'romeo_and_juliet.txt' are:\")\n",
    "for word, count in top_10_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "9. Read the [hacker news csv](https://github.com/Asabeneh/30-Days-Of-Python/blob/master/data/hacker_news.csv) file and find out:\n",
    "   a) Count the number of lines containing python or Python\n",
    "\"\"\"\n",
    "import csv\n",
    "\n",
    "file_path = \"./hacker_news.csv\"\n",
    "\n",
    "count = 0\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        if len(row) > 2 and \"python\" in row[2].lower():\n",
    "            count += 1\n",
    "\n",
    "print(\"Number of lines containing 'python' or 'Python':\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Count the number lines containing JavaScript, javascript or Javascript\n",
    "import csv\n",
    "\n",
    "file_path = \"./hacker_news.csv\"\n",
    "\n",
    "count = 0\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        try:\n",
    "            if len(row) > 2 and (\"JavaScript\" in row[2] or \"javascript\" in row[2] or \"Javascript\" in row[2]):\n",
    "                count += 1\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "print(\"Number of lines containing 'JavaScript', 'javascript', or 'Javascript':\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Count the number lines containing Java and not JavaScript\n",
    "import csv\n",
    "\n",
    "file_path = \"./hacker_news.csv\"\n",
    "\n",
    "count = 0\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        if len(row) > 2 and \"Java\" in row[2] and \"JavaScript\" not in row[2]:\n",
    "            count += 1\n",
    "\n",
    "print(\"Number of lines containing 'Java' but not 'JavaScript':\", count)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
