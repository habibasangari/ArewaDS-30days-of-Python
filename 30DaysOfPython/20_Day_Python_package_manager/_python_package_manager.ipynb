{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Define the URL\n",
    "url = 'http://www.gutenberg.org/files/1112/1112.txt'\n",
    "\n",
    "# Fetch the content from the URL\n",
    "response = requests.get(url)\n",
    "content = response.text  # Use response.text directly\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Extract the text from the HTML\n",
    "text = soup.get_text()\n",
    "\n",
    "# Remove non-alphabetic characters and convert to lowercase\n",
    "cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "\n",
    "# Split the text into words\n",
    "words = cleaned_text.split()\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Get the 10 most frequent words\n",
    "top_10_words = word_counts.most_common(10)\n",
    "\n",
    "# Print the results\n",
    "print(\"The 10 most frequent words:\")\n",
    "for word, count in top_10_words:\n",
    "    print(f\"{word} - {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import statistics\n",
    "\n",
    "def get_cat_data(api_url):\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error fetching cat data. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def extract_weights(data):\n",
    "    if data:\n",
    "        return [float(cat.get('weight').get('metric').split()[0]) for cat in data]\n",
    "    return []\n",
    "\n",
    "def calculate_statistics(weights):\n",
    "    if weights:\n",
    "        return {\n",
    "            \"Minimum\": min(weights),\n",
    "            \"Maximum\": max(weights),\n",
    "            \"Mean\": statistics.mean(weights),\n",
    "            \"Median\": statistics.median(weights),\n",
    "            \"Standard Deviation\": statistics.stdev(weights)\n",
    "        }\n",
    "    return {}\n",
    "\n",
    "def print_statistics(statistics_dict):\n",
    "    print(\"Statistical measures of cats' weight in metric units:\")\n",
    "    for measure, value in statistics_dict.items():\n",
    "        print(f\"{measure}: {value}\")\n",
    "\n",
    "# Define the URL for the Cat API\n",
    "cats_api = 'https://api.thecatapi.com/v1/breeds'\n",
    "\n",
    "# Get cat data from the API\n",
    "cat_data = get_cat_data(cats_api)\n",
    "\n",
    "# Extract weights from cat data\n",
    "cat_weights = extract_weights(cat_data)\n",
    "\n",
    "# Calculate and print statistics\n",
    "if cat_weights:\n",
    "    cat_statistics = calculate_statistics(cat_weights)\n",
    "    print_statistics(cat_statistics)\n",
    "else:\n",
    "    print(\"No cat data available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import statistics\n",
    "\n",
    "# Define the URL for the Cat API\n",
    "cats_api = 'https://api.thecatapi.com/v1/breeds'\n",
    "\n",
    "# Send GET request to the API\n",
    "response = requests.get(cats_api)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Extract the lifespans of the cats\n",
    "    lifespans = [float(cat.get('life_span', '').split()[0]) for cat in response.json() if cat.get('life_span', '').isdigit()]\n",
    "\n",
    "    # Calculate the statistical measures\n",
    "    if lifespans:\n",
    "        minimum = min(lifespans)\n",
    "        maximum = max(lifespans)\n",
    "        mean = statistics.mean(lifespans)\n",
    "        median = statistics.median(lifespans)\n",
    "        standard_deviation = statistics.stdev(lifespans)\n",
    "\n",
    "        # Print the results\n",
    "        print(\"Statistical measures of cats' lifespan in years:\")\n",
    "        print(\"Minimum:\", minimum)\n",
    "        print(\"Maximum:\", maximum)\n",
    "        print(\"Mean:\", mean)\n",
    "        print(\"Median:\", median)\n",
    "        print(\"Standard Deviation:\", standard_deviation)\n",
    "    else:\n",
    "        print(\"No valid lifespans found in the data.\")\n",
    "else:\n",
    "    print(f\"Failed to fetch cat data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from collections import Counter\n",
    "\n",
    "# Define the URL for the Cat API\n",
    "cats_api = 'https://api.thecatapi.com/v1/breeds'\n",
    "\n",
    "# Send GET request to the API\n",
    "response = requests.get(cats_api)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Extract cat data from the API response\n",
    "    data = response.json()\n",
    "\n",
    "    # Create a Counter for counting occurrences of country and breed combinations\n",
    "    frequency_table = Counter((cat.get('origin', 'Unknown'), cat.get('name', 'Unknown')) for cat in data)\n",
    "\n",
    "    # Print the frequency table\n",
    "    print(\"Frequency table of country and breed of cats:\")\n",
    "    for (country, breed), count in frequency_table.items():\n",
    "        print(f\"Country: {country} - Breed: {breed} - Count: {count}\")\n",
    "else:\n",
    "    print(f\"Failed to fetch cat data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define the URL for the Restcountries API\n",
    "countries_api = 'https://restcountries.eu/rest/v2/all'\n",
    "\n",
    "# Send GET request to the API\n",
    "response = requests.get(countries_api)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Extract data from the API response\n",
    "    data = response.json()\n",
    "\n",
    "    # Sort countries based on land area in descending order\n",
    "    sorted_countries = sorted(data, key=lambda country: country.get('area', 0), reverse=True)\n",
    "\n",
    "    # Get the 10 largest countries\n",
    "    largest_countries = sorted_countries[:10]\n",
    "\n",
    "    # Print the 10 largest countries\n",
    "    print(\"10 Largest Countries:\")\n",
    "    for country in largest_countries:\n",
    "        print(country['name'], \"-\", country.get('area', 'N/A'), \"sq km\")\n",
    "else:\n",
    "    print(f\"Failed to fetch country data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the UCI dataset page\n",
    "url = 'https://archive.ics.uci.edu/ml/datasets.php'\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, timeout=5)\n",
    "    response.raise_for_status()  # Raise an exception for bad responses\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table containing the dataset information\n",
    "table = soup.find('table', {'border': '1'})\n",
    "\n",
    "# Check if the table is found\n",
    "if table:\n",
    "    # Iterate over each row in the table\n",
    "    for row in table.find_all('tr'):\n",
    "        # Extract the dataset name and link from the table cells\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) > 0:\n",
    "            dataset_name = cells[0].text.strip()\n",
    "            dataset_link = cells[0].find('a')['href']\n",
    "            print(f\"Dataset Name: {dataset_name}\")\n",
    "            print(f\"Dataset Link: {dataset_link}\")\n",
    "            print('---')\n",
    "else:\n",
    "    print(\"Table not found on the page.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
